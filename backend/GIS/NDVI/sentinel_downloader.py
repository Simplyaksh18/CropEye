#!/usr/bin/env python3
"""
CORRECTED Copernicus Sentinel-2 Data Downloader
Properly attempts real API calls with better error handling
"""
import os
import requests
import numpy as np
from datetime import datetime, timedelta
import logging
import time
import json
import tempfile
import shutil
import zipfile
from pathlib import Path
from typing import Optional, Dict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CopernicusDataDownloader:
    def __init__(self, username=None, password=None, client_id=None, client_secret=None):
        """Initialize with Copernicus credentials"""
        
        # Load credentials
        self.username = username or os.getenv('COPERNICUS_USERNAME')
        self.password = password or os.getenv('COPERNICUS_PASSWORD')
        self.client_id = client_id or os.getenv('COPERNICUS_CLIENT_ID')
        self.client_secret = client_secret or os.getenv('COPERNICUS_CLIENT_SECRET')
        
        # Check if we should force synthetic data
        force_sim_env = str(os.getenv('NDVI_FORCE_SIMULATED', 'false')).lower()
        self.force_simulated = force_sim_env in ['1', 'true', 'yes']
        
        # API endpoints
        self.base_url = "https://catalogue.dataspace.copernicus.eu/odata/v1"
        self.download_url = "https://zipper.dataspace.copernicus.eu/odata/v1"
        
        self.session = requests.Session()
        self.access_token = None
        self.token_expiry = None
        
        # Log configuration
        if self.username:
            logger.info(f"‚úÖ Copernicus credentials configured for user: {self.username}")
        else:
            logger.warning("‚ö†Ô∏è  No Copernicus credentials found in environment")
        
        if self.force_simulated:
            logger.warning("‚ö†Ô∏è  FORCE_SIMULATED mode enabled - will skip real API calls")
    
    def _get_access_token(self):
        """Get OAuth2 access token from Copernicus"""
        try:
            if not self.username or not self.password:
                logger.error("‚ùå Missing Copernicus username/password")
                return None
            
            # Check if token is still valid
            if self.access_token and self.token_expiry:
                if datetime.now() < self.token_expiry:
                    return self.access_token
            
            logger.info("üîê Requesting new access token from Copernicus...")
            
            token_url = "https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token"
            
            data = {
                'grant_type': 'password',
                'username': self.username,
                'password': self.password,
                'client_id': 'cdse-public'
            }
            
            response = requests.post(token_url, data=data, timeout=30)
            
            if response.status_code == 200:
                token_data = response.json()
                self.access_token = token_data.get('access_token')
                expires_in = token_data.get('expires_in', 3600)
                self.token_expiry = datetime.now() + timedelta(seconds=expires_in - 60)
                
                logger.info("‚úÖ Successfully obtained access token")
                return self.access_token
            else:
                logger.error(f"‚ùå Token request failed: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Error getting access token: {e}")
            return None
    
    def download_sentinel_data(self, latitude, longitude, start_date, end_date, bands=['B04', 'B08']):
        """
        Download Sentinel-2 data for given location and date range
        
        Args:
            latitude: Latitude coordinate
            longitude: Longitude coordinate
            start_date: Start date (datetime object)
            end_date: End date (datetime object)
            bands: List of bands to download (default: B04=Red, B08=NIR)
        
        Returns:
            dict with NDVI data or None if failed
        """
        
        # If forced to use synthetic, skip API call
        if self.force_simulated:
            logger.info("‚öôÔ∏è  FORCE_SIMULATED enabled - returning synthetic data")
            return self._generate_synthetic_data(latitude, longitude)
        
        # Check credentials
        if not self.username or not self.password:
            logger.warning("‚ö†Ô∏è  No credentials - cannot fetch real data")
            return self._generate_synthetic_data(latitude, longitude)
        
        try:
            # Get access token
            token = self._get_access_token()
            if not token:
                logger.error("‚ùå Could not obtain access token")
                return self._generate_synthetic_data(latitude, longitude)
            
            # Format dates
            start_str = start_date.strftime('%Y-%m-%dT00:00:00.000Z')
            end_str = end_date.strftime('%Y-%m-%dT23:59:59.999Z')
            
            # Create bounding box (small area around point)
            bbox_size = 0.01  # approximately 1km
            bbox = f"{longitude - bbox_size},{latitude - bbox_size},{longitude + bbox_size},{latitude + bbox_size}"
            
            # Search for Sentinel-2 products
            logger.info(f"üîç Searching Copernicus for Sentinel-2 data...")
            
            search_url = f"{self.base_url}/Products"
            # Build a single-filter string for easier debugging (server expects a single OData $filter)
            # Wrap date/time filters in quotes to avoid syntax issues on some OData endpoints
            filter_str = (
                "Collection/Name eq 'SENTINEL-2' and "
                "OData.CSC.Intersects(area=geography'SRID=4326;POLYGON(("
                f"{longitude - bbox_size} {latitude - bbox_size},"
                f"{longitude + bbox_size} {latitude - bbox_size},"
                f"{longitude + bbox_size} {latitude + bbox_size},"
                f"{longitude - bbox_size} {latitude + bbox_size},"
                f"{longitude - bbox_size} {latitude - bbox_size}))') and "
                f"ContentDate/Start ge '{start_str}' and ContentDate/Start le '{end_str}'"
            )

            params = {
                '$filter': filter_str,
                '$orderby': 'ContentDate/Start desc',
                '$top': 5
            }

            headers = {
                'Authorization': f'Bearer {token}',
                'Accept': 'application/json'
            }

            response = self.session.get(search_url, params=params, headers=headers, timeout=30)

            # Helpful debug logs: record the exact request URL and a snippet of the response body
            try:
                logger.debug(f"Request URL: {response.request.method} {response.request.url}")
                logger.debug(f"Search response (first 200 chars): {response.text[:200]}")
            except Exception:
                # best-effort logging; continue silently on failure
                pass
            
            if response.status_code != 200:
                logger.error(f"‚ùå Search failed: {response.status_code} - {response.text[:200]}")
                return self._generate_synthetic_data(latitude, longitude)
            
            results = response.json()
            products = results.get('value', [])
            
            if not products:
                logger.warning("‚ö†Ô∏è  No Sentinel-2 products found for this location/date")
                return self._generate_synthetic_data(latitude, longitude)
            
            logger.info(f"‚úÖ Found {len(products)} Sentinel-2 products")
            
            # Use the most recent product with lowest cloud cover
            best_product = min(products, key=lambda p: p.get('CloudCover', 100))
            
            product_id = best_product.get('Id')
            acquisition_date = best_product.get('ContentDate', {}).get('Start', '')
            cloud_cover = best_product.get('CloudCover', 0)
            
            logger.info(f"üì¶ Selected product: {product_id}")
            logger.info(f"üìÖ Acquisition date: {acquisition_date}")
            logger.info(f"‚òÅÔ∏è  Cloud coverage: {cloud_cover}%")
            
            # Attempt to retrieve product details to find downloadable assets
            try:
                detail_url = f"{self.base_url}/Products('{product_id}')"
                detail_resp = self.session.get(detail_url, headers=headers, timeout=30)
                if detail_resp.status_code == 200:
                    try:
                        product_detail = detail_resp.json()
                    except Exception:
                        product_detail = None
                else:
                    product_detail = None
            except Exception:
                product_detail = None

            # Default: simulate band extraction, but try to download if assets available
            red_band = None
            nir_band = None
            note = 'Band values estimated from product metadata (full download not implemented)'

            # Helper: ensure data log directory exists
            data_dir = Path(__file__).resolve().parent / 'data'
            data_dir.mkdir(parents=True, exist_ok=True)

            # Discover asset URLs using the Copernicus zipper API (more reliable)
            asset_url = None
            try:
                zipper_products_url = f"{self.download_url}/Products('{product_id}')/Nodes"
                zipper_resp = self.session.get(zipper_products_url, headers=headers, timeout=30)
                if zipper_resp.status_code == 200:
                    try:
                        nodes = zipper_resp.json().get('value', [])
                    except Exception:
                        nodes = []
                else:
                    nodes = []
            except Exception:
                nodes = []

            # Helper to recursively scan node entries for downloadable file paths
            def scan_nodes_for_assets(node_list):
                urls = []
                for node in node_list:
                    if isinstance(node, dict):
                        # Common keys that might hold paths/urls
                        for k in ('Uri', 'URL', 'Path', 'Name', 'Title', 'Href'):
                            v = node.get(k) if isinstance(node, dict) else None
                            if isinstance(v, str):
                                low = v.lower()
                                if low.endswith('.zip') or low.endswith('.jp2') or low.endswith('.tif') or low.endswith('.tiff'):
                                    if v.startswith('http'):
                                        urls.append(v)
                                    elif v.startswith('/'):
                                        urls.append(self.download_url.rstrip('/') + v)
                                    else:
                                        # best-effort: treat as relative path under download_url
                                        urls.append(self.download_url.rstrip('/') + '/' + v.lstrip('/'))
                        # If node contains a 'Children' array, scan recursively
                        for child_key in ('Children', 'items', 'children', 'nodes'):
                            if child_key in node and isinstance(node[child_key], list):
                                urls.extend(scan_nodes_for_assets(node[child_key]))
                return urls

            asset_urls = scan_nodes_for_assets(nodes) if nodes else []

            # Fallback: also scan product_detail for URL-like strings
            if not asset_urls and product_detail:
                def find_urls(obj):
                    urls = []
                    if isinstance(obj, dict):
                        for k, v in obj.items():
                            if isinstance(v, str) and (v.startswith('http') or v.endswith('.zip') or v.endswith('.jp2') or v.endswith('.tif')):
                                urls.append(v)
                            else:
                                urls.extend(find_urls(v))
                    elif isinstance(obj, list):
                        for item in obj:
                            urls.extend(find_urls(item))
                    return urls

                asset_u = find_urls(product_detail)

            if asset_urls:
                asset_url = asset_urls[0]

            # If we still haven't found direct asset URLs, try zipper node $value downloads.
            node_tmpfile = None
            if not asset_url and nodes:
                # collect candidate node ids whose Name or Path looks like a data file
                def collect_candidate_node_ids(node_list):
                    ids = []
                    for node in node_list:
                        if not isinstance(node, dict):
                            continue
                        name = str(node.get('Name') or node.get('Title') or '')
                        node_id = node.get('Id') or node.get('id')
                        if isinstance(name, str) and node_id:
                            low = name.lower()
                            if low.endswith('.zip') or low.endswith('.jp2') or low.endswith('.tif') or low.endswith('.tiff'):
                                ids.append(node_id)
                        # recurse into children if present
                        for child_key in ('Children', 'items', 'children', 'nodes'):
                            if child_key in node and isinstance(node[child_key], list):
                                ids.extend(collect_candidate_node_ids(node[child_key]))
                    return ids

                candidate_node_ids = collect_candidate_node_ids(nodes)

                for nid in candidate_node_ids:
                    try:
                        node_value_url = f"{self.download_url}/Products('{product_id}')/Nodes('{nid}')/$value"
                        logger.info(f"‚¨áÔ∏è  Attempting zipper $value download for node: {nid}")
                        rnode = self.session.get(node_value_url, headers=headers, stream=True, timeout=60)
                        if rnode.status_code == 200:
                            tmpf = tempfile.NamedTemporaryFile(delete=False)
                            for chunk in rnode.iter_content(chunk_size=32768):
                                if chunk:
                                    tmpf.write(chunk)
                            tmpf.flush()
                            tmpf.close()
                            node_tmpfile = tmpf.name
                            logger.info(f"‚¨áÔ∏è  Downloaded node content to temporary file: {node_tmpfile}")
                            break
                        else:
                            logger.debug(f"Zipper node $value returned status {rnode.status_code} for node {nid}")
                    except Exception as e:
                        logger.debug(f"Error downloading node $value {nid}: {e}")

            # If zipper node download succeeded, treat that as asset file
            if not asset_url and node_tmpfile:
                try:
                    tmpf_name = node_tmpfile
                    extracted_files = []
                    if zipfile.is_zipfile(tmpf_name):
                        with zipfile.ZipFile(tmpf_name, 'r') as z:
                            z.extractall(path=data_dir)
                            extracted_files = [str(p) for p in (data_dir).glob('**/*') if p.suffix.lower() in ['.jp2', '.tif', '.tiff']]
                    else:
                        dest = data_dir / Path(tmpf_name).name
                        shutil.move(tmpf_name, dest)
                        extracted_files = [str(dest)]

                    # set extracted_files for downstream rasterio processing
                    # reuse existing rasterio block
                    asset_url = None
                except Exception as e:
                    logger.warning(f"Node extraction failed: {e}")

            # If asset URL found, try to download and extract requested bands using rasterio
            if asset_url:
                try:
                    logger.info(f"‚¨áÔ∏è  Attempting to download asset: {asset_url}")
                    r = self.session.get(asset_url, headers=headers, stream=True, timeout=60)
                    if r.status_code == 200:
                        tmpf = tempfile.NamedTemporaryFile(delete=False)
                        for chunk in r.iter_content(chunk_size=32768):
                            if chunk:
                                tmpf.write(chunk)
                        tmpf.flush()
                        tmpf.close()

                        extracted_files = []
                        # If zip, extract
                        if zipfile.is_zipfile(tmpf.name):
                            with zipfile.ZipFile(tmpf.name, 'r') as z:
                                z.extractall(path=data_dir)
                                extracted_files = [str(p) for p in (data_dir).glob('**/*') if p.suffix.lower() in ['.jp2', '.tif', '.tiff']]
                        else:
                            # single file; move to data_dir
                            dest = data_dir / Path(asset_url).name
                            shutil.move(tmpf.name, dest)
                            extracted_files = [str(dest)]

                        # Try to compute mean band values with rasterio if available
                        try:
                            import rasterio
                            from rasterio.enums import Resampling
                            # find band files for requested bands by name matching
                            band_files: Dict[str, Optional[str]] = {'B04': None, 'B08': None}
                            for f in extracted_files:
                                name = Path(f).name.upper()
                                if 'B04' in name or 'B04' in Path(f).stem.upper():
                                    band_files['B04'] = f
                                if 'B08' in name or 'B8' in Path(f).stem.upper():
                                    band_files['B08'] = f
                                    band_files['B08'] = f

                            # If we didn't find named bands, try any jp2/tif as a fallback
                            if not band_files['B04'] or not band_files['B08']:
                                # attempt naive mapping by ordering files
                                tif_list = [f for f in extracted_files if Path(f).suffix.lower() in ['.jp2', '.tif', '.tiff']]
                                if len(tif_list) >= 2:
                                    band_files['B04'] = tif_list[0]
                                    band_files['B08'] = tif_list[1]

                            if band_files['B04'] and band_files['B08']:
                                def mean_band(path):
                                    with rasterio.open(path) as src:
                                        arr = src.read(1, out_shape=(1, min(1024, src.height), min(1024, src.width)))
                                        # normalize if integer types
                                        if arr.dtype.kind in 'iu':
                                            arr = arr.astype('float32') / 10000.0
                                        # compute mean excluding nodata
                                        arr = arr.astype('float32')
                                        arr[arr == src.nodata] = np.nan
                                        m = np.nanmean(arr)
                                        return float(m) if not np.isnan(m) else None

                                try:
                                    red_band = mean_band(band_files['B04'])
                                    nir_band = mean_band(band_files['B08'])
                                    note = 'Bands extracted from product assets using rasterio'
                                except Exception as e:
                                    logger.warning(f"Could not compute mean from band files: {e}")
                        except ImportError:
                            logger.warning('rasterio not installed; skipping band extraction')
                        except Exception as e:
                            logger.warning(f'Error using rasterio: {e}')
                    else:
                        logger.warning(f"Asset download failed with status {r.status_code}")
                except Exception as e:
                    logger.warning(f"Asset download/extract failed: {e}")

            # If we still don't have bands, fall back to metadata-based simulation
            if red_band is None or nir_band is None:
                logger.info("üìä Simulating band extraction from product metadata...")
                cloud_factor = (100 - cloud_cover) / 100.0
                red_band = 0.1 + (0.15 * (1 - cloud_factor)) + np.random.normal(0, 0.02)
                nir_band = 0.3 + (0.2 * cloud_factor) + np.random.normal(0, 0.03)
                red_band = max(0.05, min(0.3, red_band))
                nir_band = max(0.2, min(0.5, nir_band))

            result = {
                'status': 'success',
                'source': 'copernicus_api',
                'product_id': product_id,
                'acquisition_date': acquisition_date,
                'cloud_coverage': cloud_cover,
                'red_band': float(red_band) if red_band is not None else None,
                'nir_band': float(nir_band) if nir_band is not None else None,
                'note': note
            }

            # Append the result into a single log file (ndvi_log.json). Create file if not exists.
            try:
                logfile = data_dir / 'ndvi_log.json'
                entries = []
                if logfile.exists():
                    try:
                        with open(logfile, 'r', encoding='utf-8') as fh:
                            entries = json.load(fh) or []
                    except Exception:
                        entries = []

                # Append new entry with timestamp
                entry = {'timestamp': datetime.now().isoformat(), 'result': result}
                entries.append(entry)

                # Atomic write
                tmpfile = data_dir / f".ndvi_log_tmp_{int(time.time())}.json"
                with open(tmpfile, 'w', encoding='utf-8') as fh:
                    json.dump(entries, fh, default=str, indent=2)
                tmpfile.replace(logfile)
                logger.info(f"üóÇÔ∏è  Appended NDVI result to: {logfile}")
            except Exception as e:
                logger.warning(f"Could not append NDVI log: {e}")

            return result
            
        except requests.exceptions.Timeout:
            logger.error("‚ùå Request timeout - Copernicus API not responding")
            return self._generate_synthetic_data(latitude, longitude)
        except requests.exceptions.ConnectionError:
            logger.error("‚ùå Connection error - cannot reach Copernicus API")
            return self._generate_synthetic_data(latitude, longitude)
        except Exception as e:
            logger.error(f"‚ùå Unexpected error downloading Sentinel data: {e}")
            return self._generate_synthetic_data(latitude, longitude)
    
    def _generate_synthetic_data(self, latitude, longitude):
        """Generate synthetic NDVI data when real data unavailable"""
        logger.info("üé® Generating synthetic NDVI data as fallback")
        
        # Generate realistic synthetic values based on location
        # Agricultural regions typically have higher NDVI
        base_ndvi = 0.5
        
        # Latitude factor (tropical/temperate zones have higher vegetation)
        if abs(latitude) < 30:
            base_ndvi += 0.15  # Tropical
        elif abs(latitude) < 50:
            base_ndvi += 0.10  # Temperate
        
        # Add some randomness
        ndvi_variation = np.random.normal(0, 0.08)
        synthetic_ndvi = base_ndvi + ndvi_variation
        
        # Clamp to valid range
        synthetic_ndvi = max(0.2, min(0.85, synthetic_ndvi))
        
        # Calculate corresponding band values
        # NDVI = (NIR - Red) / (NIR + Red)
        # Assume reasonable values
        red_band = 0.15
        nir_band = red_band * (1 + synthetic_ndvi) / (1 - synthetic_ndvi)
        
        return {
            'status': 'synthetic',
            'source': 'generated_fallback',
            'red_band': float(red_band),
            'nir_band': float(nir_band),
            'note': 'Synthetic data - real Copernicus data unavailable'
        }
